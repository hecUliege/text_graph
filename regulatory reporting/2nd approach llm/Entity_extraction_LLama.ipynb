{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77a569-76cd-49ff-b4da-c29dc1f9fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d712a5-d053-4bf1-92bc-5bd2f562e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f408de-89ef-4acb-9e0f-fe02124d0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adf5b7-fc1a-4f20-aae5-e287f725c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009efdc-d510-4a98-ad01-5482c4278073",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        # quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7730f-8a12-4093-8e4b-7fdc89d91e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def prompting(sentence: str, output_log_filename, error_log_filename):\n",
    "    #1st approach to few shot promting , whereby system (instruction), user (user input) and llm response are combined\n",
    "\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a virtual annotation. For each sentence, you annotate the addresser, addressee, actionResult, date. \n",
    "    The output should be in Json format.\n",
    "    Adresser: Who performs the action.\n",
    "    Action: What action is performed.\n",
    "    ActionResult: What is done.\n",
    "    Addressee: To whom the action is directed.\n",
    "    Date: When the action occurs/deadline.\"\"\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"\"\"By 31 December 2010 and, thereafter, at least every three years, the Commission shall review the provisions concerning its implementing powers and present a report to the European Parliament and to the Council on the functioning of those powers. The report shall examine, in particular, the need for the Commission to propose amendments to this Directive in order to ensure the appropriate scope of the implementing powers conferred on the Commission. The conclusion as to whether or not an amendment is necessary shall be accompanied by a detailed statement of reasons. If necessary, the report shall be accompanied by a legislative proposal to amend the provisions conferring implementing powers on the Commission.\"\"\"},\n",
    "    {\"role\": \"system\", \"content\": \"\"\"{\n",
    "    \"addresser\": [\"the commission\"],\n",
    "    \"Action\": [\"present\"],\n",
    "    \"ActionResult\": [\"a report on the functioning of those powers\"],\n",
    "    \"addressee\": [\"the european parliament\", \"the council\"],\n",
    "    \"Date\": [\"By 31 December 2010\"]\n",
    "    }\"\"\"},\n",
    "    \n",
    "     {\"role\": \"user\", \"content\": \"\"\"Member States shall communicate to the Commission the texts of the main provisions of national law which they adopt in the field governed by this Directive.\"\"\"},\n",
    "    {\"role\": \"system\", \"content\": \"\"\"{\n",
    "    \"addresser\": [\"member states\"],\n",
    "    \"Action\": [\"communicate\"],\n",
    "    \"ActionResult\": [\"the texts of the main provisions of national law\"],\n",
    "    \"addressee\": [\"the Commission\"],\n",
    "    \"Date\": [\"None\"]\n",
    "    }\"\"\"},\n",
    "    \n",
    "     {\"role\": \"user\", \"content\": \"\"\"Upon reasoned request, Member States shall forthwith communicate the reports referred to in Article 111(3) to the competent authorities of another Member State.\"\"\"},\n",
    "    {\"role\": \"system\", \"content\": \"\"\"{\n",
    "    \"addresser\": [\"Member States\"],\n",
    "    \"Action\": [\"communicate\"],\n",
    "    \"ActionResult\": [\"the reports referred to in Article 111(3)\"],\n",
    "    \"addressee\": [\"the competent authorities of another Member State\"],\n",
    "    \"Date\": [\"forthwith\"]\n",
    "    }\"\"\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": \"\"\"Each year the sponsor shall submit to the Agency a report on the state of development of the designated medicinal product.\"\"\"},\n",
    "    {\"role\": \"system\", \"content\": \"\"\"{\n",
    "    \"addresser\": [\"the sponsor\"],\n",
    "    \"Action\": [\"submit\"],\n",
    "    \"ActionResult\": [\"a report on the state of development of the designated medicinal product\"],\n",
    "    \"addressee\": [\"the Agency\"],\n",
    "    \"Date\": [\"Each year\"]\n",
    "    }\"\"\"},\n",
    "    \n",
    "    {\"role\": \"user\", \"content\": sentence},\n",
    "]\n",
    "    # inputs = tokenizer(llama_prompt_tempate, return_tensors=\"pt\").input_ids.cuda()\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    # print(\"size of input: \", inputs[0].shape)\n",
    "    # print('***Decode Input:\\n', tokenizer.decode(inputs[0]))\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=0\n",
    "            # do_sample=True,\n",
    "            # temperature=0.6,\n",
    "            # top_p=0.9,\n",
    "        )\n",
    "   \n",
    "        response = outputs[0][inputs.shape[-1]:]\n",
    "        response_str = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        \n",
    "        return parseJson(response_str, sentence, output_log_filename, error_log_filename)\n",
    "\n",
    "def parseJson(llm_response, sentence, output_filename, error_filename):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        parsed_json = json.loads(llm_response.strip().lower())\n",
    "        saveToExcelFile(sentence, llm_response, output_filename)\n",
    "        \n",
    "    except json.JSONDecodeError as e:      \n",
    "        \n",
    "        saveToExcelFile(sentence, llm_response, error_filename)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def saveToFile(log, filename, mode = \"w+\"):\n",
    "\n",
    "    with open(\"data/llm_output/\" + filename + \".txt\", mode) as f:\n",
    "        f.write(log)\n",
    "        f.close()\n",
    "    \n",
    "def saveToExcelFile(sentences, responses, save_dir, writing_mode = \"w+\"):\n",
    "    df = pd.DataFrame({'Sentence': [sentences], 'Ouput': [responses]})\n",
    "        # Check if the file exists\n",
    "    filename = \"data/llm_output/\" + save_dir + \".xlsx\"\n",
    "    if not os.path.exists(filename):\n",
    "        print('file not existing')\n",
    "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "    else:\n",
    "        print('file exist')\n",
    "        # Append to the existing file\n",
    "        with pd.ExcelWriter(filename, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "            df.to_excel(writer, sheet_name='Sheet1', index=False, header=False, startrow=writer.sheets['Sheet1'].max_row if 'Sheet1' in writer.sheets else 0)\n",
    "    \n",
    "\n",
    "def main():\n",
    "\n",
    "    df = pd.read_excel(\"data/manually_selection.xlsx\", sheet_name = \"Sheet1\")\n",
    "    \n",
    "    output_log_filename = \"llama_output_chattemplate_temperature_0_evaluation\"\n",
    "    error_log_filename = \"error_prompt_chattemplate_temperature_0_evaluation\"\n",
    "   \n",
    "   \n",
    "    # Iterate over the dataframe\n",
    "    for _, paragraph in df.iterrows():\n",
    "        requirement = prompting(paragraph['Sentence'], output_log_filename, error_log_filename)\n",
    "    print(\"### Finish ###\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3134a-cd71-45c0-bc64-aa685bf9deff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
