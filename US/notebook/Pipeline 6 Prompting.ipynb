{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name= \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"microsoft/phi-2\"\n",
    "# model_name = \"intfloat/e5-mistral-7b-instruct\"\n",
    "save_name = \"Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n",
    "# tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_few_shot_promt(training_set, testing: str):\n",
    "    # Convert dataset to pandas DataFrame\n",
    "    df = pd.DataFrame(training_set)\n",
    "\n",
    "    # Example DataFrame with 'class' column\n",
    "    # Replace 'class_column_name' with the actual name of the column that represents the class\n",
    "    class_column_name = 'Label'\n",
    "\n",
    "    # Randomly choose three datasets from each class\n",
    "    random_samples = df.groupby(class_column_name).apply(lambda x: x.sample(n=min(3, len(x))))\n",
    "   \n",
    "    return promt_generation3(random_samples, testing)\n",
    "    # return promt_mistral(random_samples, testing)\n",
    "   \n",
    "\n",
    "def promt_generation3(sample, testing):\n",
    "    # instruction = \"You are a classification model. Based on the input, you need to predict the most relevant category label from {Hard-goal, Soft-goal, Task, Capability}. If the text doesn't fit into any of the above categories, classify it as Hard-goal. Each input has only one label. \\n\"\n",
    "    instruction = \"You are a classification model. Based on the input, you need to predict the most relevant category label from {HG, SG, Task, Cap}. If the text doesn't fit into any of the above categories, classify it as HG. Each input has only one label. \\n\"\n",
    "    prompt_training_data = \"\"\n",
    "    instruct = f\"### Instruction\\n{instruction}\"\n",
    "    for _, i in sample.iterrows():\n",
    "        # print(i)\n",
    "        context = f\"### Input\\n{i['UserStory']} \\n\" \n",
    "        response = f\"### Output\\n {i['Label']} \\n\"\n",
    "        prompt_training_data = prompt_training_data + context + response\n",
    "    test_prompt =  f\"### Input\\n{testing} \\n\" + f\"### Output\\n\"\n",
    "    return instruct + prompt_training_data + test_prompt\n",
    "\n",
    "def promt_mistral(sample, testing):\n",
    "    \n",
    "    prompt_training_data = \"\"\n",
    "    for _, i in sample.iterrows():\n",
    "        context = f\"Inquiry:\\n{i['UserStory']} \\n\" \n",
    "        response = f\"Category:\\n {i['Label']} \\n\"\n",
    "        prompt_training_data = prompt_training_data + context + response\n",
    "    \n",
    "    user_message = (\n",
    "        f\"\"\"\n",
    "        You are a agile bot in software development. Your task is to assess developer to categorize inquiry after <<<>>> into one of the following predefined categories:\n",
    "        \n",
    "        Hard-goal\n",
    "        Soft-goal\n",
    "        Task\n",
    "        Capability\n",
    "        \n",
    "        If the text doesn't fit into any of the above categories, classify it as:\n",
    "        Hard-goal\n",
    "        \n",
    "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes. \n",
    "        \n",
    "        ####\n",
    "        Here are some examples:\n",
    "        \n",
    "        {prompt_training_data}\n",
    "        ###\n",
    "    \n",
    "        <<<\n",
    "        Inquiry: {testing}\n",
    "        Category:\n",
    "        >>>\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    return user_message\n",
    "\n",
    "def decoder(prompt_mss):\n",
    "    model_inputs = tokenizer(prompt_mss, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=1, temperature=0.1 , do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    # print(tokenizer.batch_decode(generated_ids)[0])\n",
    "    return tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "def getLabelFromPrompt(result):\n",
    "    pattern = \"### Output\\n\" \n",
    "    # pattern = \"Category:\" # for Mistral Prompt Template\n",
    "    output = result.split(pattern)\n",
    "    return output[len(output)-1]\n",
    "\n",
    "def correct_label(label):\n",
    "   \n",
    "    if label.strip() == 'Cap': return \"Capability\"    \n",
    "    elif label.strip() == 'Hard': return \"Hard-goal\"\n",
    "    elif label.strip() == \"Soft\": return \"Soft-goal\"\n",
    "    else: return label.strip()\n",
    "    \n",
    "# target_names = ['Capability', 'Hard-goal', 'Soft-goal', 'Task']\n",
    "target_names = ['Cap', 'HG', 'SG', 'Task']\n",
    "def verify_inconsistency_label(training_set, testing):\n",
    "    prompt_mss = make_few_shot_promt(training_set, testing)\n",
    "    result_mss = decoder(prompt_mss)\n",
    "    # label_tmp = correct_label(getLabelFromPrompt(result_mss))\n",
    "    label_tmp = getLabelFromPrompt(result_mss)\n",
    "    \n",
    "    if label_tmp not in target_names:\n",
    "        print('Label Not found')\n",
    "        return verify_inconsistency_label(training_set, testing)\n",
    "    else: return label_tmp\n",
    "    \n",
    "def evaluation(labels, preds, target_names):\n",
    "    metricReport = classification_report(labels, preds, target_names=target_names, zero_division=0, output_dict=True)\n",
    "    return {\n",
    "        'Accuracy': metricReport['accuracy'],\n",
    "        'CapP': metricReport[target_names[0]]['precision'],\n",
    "        'CapR': metricReport[target_names[0]]['recall'],              \n",
    "        'CapF1': metricReport[target_names[0]]['f1-score'],\n",
    "        'HGP': metricReport[target_names[1]]['precision'],\n",
    "        'HGR': metricReport[target_names[1]]['recall'],\n",
    "        'HGF1': metricReport[target_names[1]]['f1-score'],\n",
    "        'SGP': metricReport[target_names[2]]['precision'],\n",
    "        'SGR': metricReport[target_names[2]]['recall'],\n",
    "        'SGF1': metricReport[target_names[2]]['f1-score'],\n",
    "        'TP': metricReport[target_names[3]]['precision'],\n",
    "        'TR': metricReport[target_names[3]]['recall'],\n",
    "        'TF1': metricReport[target_names[3]]['f1-score'],\n",
    "    }\n",
    "    \n",
    "def promt_qa_generation(sample, testing):\n",
    "    instruction = \"You are a classification model. Based on the input, you need to predict the most relevant category label from {Hard-goal, Soft-goal, Task, Capability}. If the text doesn't fit into any of the above categories, classify it as Hard-goal. Each input has only one label. \\n\"\n",
    "    prompt_training_data = \"\"\n",
    "    instruct = f\"### Instruction\\n{instruction}\"\n",
    "    for _, i in sample.iterrows():\n",
    "        # print(i)\n",
    "        context = f\"### Input\\n{i['UserStory']} \\n\" \n",
    "        response = f\"### Output\\n {i['Label']} \\n\"\n",
    "        prompt_training_data = prompt_training_data + context + response\n",
    "    test_prompt =  f\"### Input\\n{testing} \\n\" + f\"### Output\\n\"\n",
    "    return instruct + prompt_training_data + test_prompt\n",
    "    \n",
    "def changeLabel(label):\n",
    "    \n",
    "    replace_dict = {\n",
    "        \"Capability\": \"Cap\",\n",
    "        \"Hard-goal\": \"HG\",\n",
    "        \"Soft-goal\": \"SG\",\n",
    "        \"Task\": \"Task\",\n",
    "    }\n",
    "    # updated_labels = [replace_dict.get(item) for item in labels]\n",
    "    \n",
    "    label['Label'] = replace_dict.get(label['Label'])\n",
    "    return {'Label': label['Label']}\n",
    "    # return replace_dict.get(label)\n",
    "# print(make_few_shot_promt(dataset['train'], \"i want to see correct status labels on the submission dashboard\"))\n",
    "# prompt_mss = make_few_shot_promt(dataset['train'], \"i want to see correct status labels on the submission dashboard\")\n",
    "# print(getLabelFromPrompt(decoder(prompt_mss)).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"../data/us/newDataset/separate_5_folds_2/\"\n",
    "results = []\n",
    "for iteratorDataset in range(1, 6): \n",
    "    dataset = load_dataset('csv', data_files={'train': data_dir +  'train_' + str(iteratorDataset) + '.csv', 'test': data_dir + 'test_' + str(iteratorDataset) + '.csv'}, encoding = \"utf-8\")\n",
    "    dataset = dataset.remove_columns(['Unnamed: 0'])\n",
    "    dataset = dataset.map(changeLabel)\n",
    "    y_pred = []\n",
    "    for i in dataset['test']:\n",
    "        # print('****')\n",
    "        # print(make_few_shot_promt(dataset['train'], i['UserStory']))\n",
    "        # print('Gold Label: ', i['UserStory'], i['Label'])\n",
    "        \n",
    "        # prompt_mss = make_few_shot_promt(dataset['train'], i['UserStory'])\n",
    "        # result_mss = decoder(prompt_mss)\n",
    "        # label_tmp = correct_label(getLabelFromPrompt(result_mss))\n",
    "        \n",
    "        # if label_tmp not in target_names:\n",
    "        #     print(result_mss)\n",
    "        #     prompt_mss = make_few_shot_promt(dataset['train'], i['UserStory'])\n",
    "        #     result_mss = decoder(prompt_mss)\n",
    "        #     label_tmp = getLabelFromPrompt(result_mss)\n",
    "        # else:\n",
    "        # print(result_mss)\n",
    "        label_tmp = verify_inconsistency_label(dataset['train'], i['UserStory'])\n",
    "        y_pred.append(label_tmp)\n",
    "    print(classification_report(dataset['test']['Label'], y_pred))\n",
    "    # results.append(evaluation(dataset['test']['Label'], y_pred, target_names=target_names))\n",
    "    print(y_pred)\n",
    "    \n",
    "    \n",
    "# pd.DataFrame(results).mean().to_excel('../results/'+ save_name +'.xlsx')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
