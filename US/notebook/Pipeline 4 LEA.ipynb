{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378adf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\porch\\.conda\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe215823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\porch\\.conda\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\porch\\.cache\\huggingface\\hub\\models--sentence-transformers--bert-base-nli-mean-tokens. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/nli-bert-base')\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966b4d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0.8112252441444138\n",
      "0.7766423357664234\n",
      "0.7931026255196353\n",
      "0.16499999999999998\n",
      "0.175\n",
      "0.16576923076923078\n",
      "0.47864347776462024\n",
      "0.4666666666666666\n",
      "0.47094884945631216\n",
      "0.2787719298245614\n",
      "0.38888888888888884\n",
      "0.32214513844948633\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    path = r\"../data/us/newDataset/separate_5_folds_2/\"\n",
    "    train_df = pd.read_csv(path + '\\\\train_' + str(i) + '.csv')\n",
    "    eval_df = pd.read_csv(path + '\\\\test_' + str(i) + '.csv')\n",
    "    text_col=train_df.columns.values[1] \n",
    "    category_col=train_df.columns.values[2]\n",
    "\n",
    "    x_eval = eval_df[text_col].values.tolist()\n",
    "    y_eval = eval_df[category_col].values.tolist()\n",
    "\n",
    "    x_train = train_df[text_col].values.tolist()\n",
    "    y_train = train_df[category_col].values.tolist()\n",
    "    \n",
    "    training_embeddings = model.encode(x_train)\n",
    "    testing_embeddings = model.encode(x_eval)\n",
    "    print(i)\n",
    "    yPred = []\n",
    "    for i in testing_embeddings:\n",
    "        _, index = findMaxVec2(i, training_embeddings)\n",
    "        yPred.append(y_train[index[0][1]])\n",
    "#     print(classification_report(y_eval, yPred))\n",
    "    results.append(evaluation(y_eval, yPred))\n",
    "    \n",
    "CapR = CapP = CapF1 = HGP = HGR = HGF1 = SGP = SGR = SGF1 = TP = TR = TF1 = 0\n",
    "for i in results:\n",
    "    CapP += i['CapP']\n",
    "    CapR += i['CapR']\n",
    "    CapF1 += i['CapF1']\n",
    "    HGP += i['HGP']\n",
    "    HGR += i['HGR']\n",
    "    HGF1 += i['HGF1']\n",
    "    SGP += i['SGP']\n",
    "    SGR += i['SGR']\n",
    "    SGF1 += i['SGF1']\n",
    "    TP += i['TP']\n",
    "    TR += i['TR']\n",
    "    TF1 += i['TF1']\n",
    "print(CapP/len(results))\n",
    "print(CapR/len(results))\n",
    "print(CapF1/len(results))\n",
    "\n",
    "print(HGP/len(results))\n",
    "print(HGR/len(results))\n",
    "print(HGF1/len(results))\n",
    "\n",
    "print(SGP/len(results))\n",
    "print(SGR/len(results))\n",
    "print(SGF1/len(results))\n",
    "\n",
    "print(TP/len(results))\n",
    "print(TR/len(results))\n",
    "print(TF1/len(results))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(testing_embeddings[0]))\n",
    "# for i in range(len(testing_embeddings) - 1):\n",
    "#     print(len(testing_embeddings[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc69039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxVec(sent, labels_embeddings):\n",
    "    similarities = cosine_similarity(model.encode([sent]), labels_embeddings)\n",
    "    max_vector = max(similarities[0])\n",
    "    # return array because similarites is 2D array\n",
    "    max_index = np.argwhere(similarities == max_vector)\n",
    "    return max_vector, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a9784d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMaxVec2(testing_embeddings, training_embeddings):\n",
    "    similarities = cosine_similarity([testing_embeddings], training_embeddings)\n",
    "    max_vector = max(similarities[0])\n",
    "    # return array because similarites is 2D array\n",
    "    max_index = np.argwhere(similarities == max_vector)\n",
    "    return max_vector, max_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47bd4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(findMaxVec2(testing_embeddings[0], training_embeddings))\n",
    "# print(findMaxVec(x_eval[0], training_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e452d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(labels, preds, target_names = None):\n",
    "    target_names = ['Capability', 'Hard-goal', 'Soft-goal', 'Task']\n",
    "    metricReport = classification_report(labels, preds, zero_division=0, output_dict=True)\n",
    "    return {\n",
    "        'Accuracy': metricReport['accuracy'],\n",
    "        'CapP': metricReport[target_names[0]]['precision'],\n",
    "        'CapR': metricReport[target_names[0]]['recall'],              \n",
    "        'CapF1': metricReport[target_names[0]]['f1-score'],\n",
    "        'HGP': metricReport[target_names[1]]['precision'],\n",
    "        'HGR': metricReport[target_names[1]]['recall'],\n",
    "        'HGF1': metricReport[target_names[1]]['f1-score'],\n",
    "        'SGP': metricReport[target_names[2]]['precision'],\n",
    "        'SGR': metricReport[target_names[2]]['recall'],\n",
    "        'SGF1': metricReport[target_names[2]]['f1-score'],\n",
    "        'TP': metricReport[target_names[3]]['precision'],\n",
    "        'TR': metricReport[target_names[3]]['recall'],\n",
    "        'TF1': metricReport[target_names[3]]['f1-score'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1ce8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
